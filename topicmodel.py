# -*- coding: utf-8 -*-
"""topicmodel.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MVMIEkHoN1t7Rgoj7Lh4EyWX1CSgJSAC
"""

import streamlit as st
import pandas as pd
from PIL import Image
from io import BytesIO
import requests
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation

# Set page title and icon
st.set_page_config(page_title="Text Analyser", page_icon=":pencil:")

# Set app name and subheading
st.title("Text Analyser")
st.subheader("Developer: Sunil Kappal")

# Upload logo from URL
logo_url = st.text_input("Upload Logo URL:")
if logo_url:
    logo = Image.open(BytesIO(requests.get("https://humach.com/wp-content/uploads/2023/01/HuMach_logo-bold.png").content))
    st.image(logo, caption='Uploaded Logo', use_column_width=True)

# Upload file for analysis
uploaded_file = st.file_uploader("Upload CSV or Text file for analysis", type=["csv", "txt"])

if uploaded_file is not None:
    # Read the file
    if uploaded_file.name.endswith('.csv'):
        df = pd.read_csv(uploaded_file)
    elif uploaded_file.name.endswith('.txt'):
        df = pd.DataFrame({'Text': uploaded_file.readlines()})
    else:
        st.error("Invalid file format. Please upload a CSV or Text file.")
        st.stop()

    # Display uploaded data
    st.write("Uploaded Data:")
    st.dataframe(df)

    # Perform text analytics
    st.header("Text Analytics:")
    # Add your text analytics code here using libraries like TextBlob, NLTK, etc.

    # Perform topic modelling
    st.header("Topic Modelling:")
    num_topics = st.slider("Select the number of topics", min_value=2, max_value=10, value=4)

    # Topic Modelling using scikit-learn's LatentDirichletAllocation
    vectorizer = CountVectorizer()
    X = vectorizer.fit_transform(df['Text'])
    lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)
    lda.fit(X)

    # Display topic modelling results
    for topic_idx, topic in enumerate(lda.components_):
        st.write(f"Topic {topic_idx + 1}:")
        top_words_idx = topic.argsort()[-5:][::-1]
        top_words = [vectorizer.get_feature_names_out()[i] for i in top_words_idx]
        st.write(", ".join(top_words))

    # Assign topics to each document
    df['Topic'] = lda.transform(X).argmax(axis=1)
    st.write("Assigned Topics:")
    st.dataframe(df[['Text', 'Topic']])

